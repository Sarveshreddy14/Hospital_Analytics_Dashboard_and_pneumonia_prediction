{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c0ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7, 144)\n",
      "\n",
      "Missing values:\n",
      " row_id          0\n",
      "subject_id      0\n",
      "hadm_id         0\n",
      "icustay_id      0\n",
      "dbsource        0\n",
      "               ..\n",
      "std_map_24h     6\n",
      "std_rr_24h      6\n",
      "std_sbp_24h     7\n",
      "std_spo2_24h    6\n",
      "std_temp_24h    7\n",
      "Length: 144, dtype: int64\n",
      "\n",
      "Class distribution before splitting:\n",
      "label_mortality\n",
      "0    6\n",
      "1    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after splitting:\n",
      "Train: [4 1]\n",
      "Test: [2]\n",
      "\n",
      "=== Mortality Classification ===\n",
      "Accuracy: 0.000\n",
      "F1-score: 0.000\n",
      "ROC-AUC: nan\n",
      "\n",
      "=== Length of Stay Regression ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\impute\\_base.py:637: UserWarning: Skipping features without any observed values: ['count_dbp_24h' 'count_sbp_24h' 'count_temp_24h' 'last_dbp_24h'\n",
      " 'last_sbp_24h' 'last_temp_24h' 'max_dbp_24h' 'max_sbp_24h' 'max_temp_24h'\n",
      " 'mean_dbp_24h' 'mean_sbp_24h' 'mean_temp_24h' 'min_dbp_24h' 'min_sbp_24h'\n",
      " 'min_temp_24h' 'std_dbp_24h' 'std_sbp_24h' 'std_temp_24h']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 47.80 days (baseline: 43.82 days)\n",
      "RMSE: 6.91 days\n",
      "R²: -0.190\n",
      "\n",
      "=== Patient Clustering ===\n",
      "Optimal number of clusters: 2\n",
      "Best silhouette score: 0.164\n",
      "Best Calinski-Harabasz score: 2.2\n",
      "\n",
      "Cluster Profiles:\n",
      "           age  count_bicarbonate_24h  count_bun_24h  count_chloride_24h  \\\n",
      "cluster                                                                    \n",
      "0        59.25                    4.0            4.0                 4.0   \n",
      "1        74.58                    1.2            1.4                 1.2   \n",
      "\n",
      "         count_creatinine_24h  count_glucose_24h  count_hgb_24h  \\\n",
      "cluster                                                           \n",
      "0                         4.0                4.5            7.5   \n",
      "1                         1.4                1.2            1.8   \n",
      "\n",
      "         count_lactate_24h  count_platelets_24h  count_potassium_24h  ...  \\\n",
      "cluster                                                               ...   \n",
      "0                     12.0                  4.0                  5.0  ...   \n",
      "1                      1.5                  1.4                  1.2  ...   \n",
      "\n",
      "         min_sbp_24h  min_spo2_24h  min_temp_24h  std_dbp_24h  std_hr_24h  \\\n",
      "cluster                                                                     \n",
      "0                NaN           NaN           NaN          NaN         NaN   \n",
      "1                NaN          97.0           NaN          NaN    4.362922   \n",
      "\n",
      "         std_map_24h  std_rr_24h  std_sbp_24h  std_spo2_24h  std_temp_24h  \n",
      "cluster                                                                    \n",
      "0                NaN         NaN          NaN           NaN           NaN  \n",
      "1           9.635189    4.088924          NaN      0.998365           NaN  \n",
      "\n",
      "[2 rows x 109 columns]\n",
      "\n",
      "=== Association Rules Mining ===\n",
      "Filtered from 271 to 183 diagnoses\n",
      "Transaction matrix shape: (129, 183)\n",
      "Mining frequent itemsets...\n",
      "Found 3716 frequent itemsets\n",
      "Generating association rules...\n",
      "Optimal number of clusters: 2\n",
      "Best silhouette score: 0.164\n",
      "Best Calinski-Harabasz score: 2.2\n",
      "\n",
      "Cluster Profiles:\n",
      "           age  count_bicarbonate_24h  count_bun_24h  count_chloride_24h  \\\n",
      "cluster                                                                    \n",
      "0        59.25                    4.0            4.0                 4.0   \n",
      "1        74.58                    1.2            1.4                 1.2   \n",
      "\n",
      "         count_creatinine_24h  count_glucose_24h  count_hgb_24h  \\\n",
      "cluster                                                           \n",
      "0                         4.0                4.5            7.5   \n",
      "1                         1.4                1.2            1.8   \n",
      "\n",
      "         count_lactate_24h  count_platelets_24h  count_potassium_24h  ...  \\\n",
      "cluster                                                               ...   \n",
      "0                     12.0                  4.0                  5.0  ...   \n",
      "1                      1.5                  1.4                  1.2  ...   \n",
      "\n",
      "         min_sbp_24h  min_spo2_24h  min_temp_24h  std_dbp_24h  std_hr_24h  \\\n",
      "cluster                                                                     \n",
      "0                NaN           NaN           NaN          NaN         NaN   \n",
      "1                NaN          97.0           NaN          NaN    4.362922   \n",
      "\n",
      "         std_map_24h  std_rr_24h  std_sbp_24h  std_spo2_24h  std_temp_24h  \n",
      "cluster                                                                    \n",
      "0                NaN         NaN          NaN           NaN           NaN  \n",
      "1           9.635189    4.088924          NaN      0.998365           NaN  \n",
      "\n",
      "[2 rows x 109 columns]\n",
      "\n",
      "=== Association Rules Mining ===\n",
      "Filtered from 271 to 183 diagnoses\n",
      "Transaction matrix shape: (129, 183)\n",
      "Mining frequent itemsets...\n",
      "Found 3716 frequent itemsets\n",
      "Generating association rules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarve\\AppData\\Roaming\\Python\\Python311\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 20 strong association rules\n",
      "\n",
      "Top Association Rules:\n",
      "     antecedents consequents  support  confidence   lift\n",
      "532   (253, 707)       (227)    0.023        1.00  43.00\n",
      "533        (227)  (253, 707)    0.023        1.00  43.00\n",
      "5270  (572, 585)       (452)    0.023        1.00  43.00\n",
      "5273       (452)  (572, 585)    0.023        1.00  43.00\n",
      "16         (227)       (253)    0.023        1.00  32.25\n",
      "531   (227, 707)       (253)    0.023        1.00  32.25\n",
      "3722  (438, 443)       (344)    0.023        1.00  32.25\n",
      "4274  (397, V58)       (396)    0.023        1.00  32.25\n",
      "5258       (452)  (585, 571)    0.023        1.00  32.25\n",
      "5262       (452)  (785, 571)    0.023        1.00  32.25\n",
      "5266       (452)  (571, 995)    0.023        1.00  32.25\n",
      "5277       (452)  (572, 785)    0.023        1.00  32.25\n",
      "5294       (452)  (785, 585)    0.023        1.00  32.25\n",
      "17         (253)       (227)    0.023        0.75  32.25\n",
      "534        (253)  (227, 707)    0.023        0.75  32.25\n",
      "3725       (344)  (438, 443)    0.023        0.75  32.25\n",
      "4277       (396)  (397, V58)    0.023        0.75  32.25\n",
      "5255  (585, 571)       (452)    0.023        0.75  32.25\n",
      "5259  (785, 571)       (452)    0.023        0.75  32.25\n",
      "5263  (571, 995)       (452)    0.023        0.75  32.25\n",
      "\n",
      "Top Rules with Labels:\n",
      "                                    antecedents_labeled  \\\n",
      "532   (253 (Acromegaly and gigantism), 707 (Pressure...   \n",
      "533                     (227 (Benign neoplasm adrenal))   \n",
      "5270  (572 (Abscess of liver), 585 (Chro kidney dis ...   \n",
      "5273                     (452 (Portal vein thrombosis))   \n",
      "16                      (227 (Benign neoplasm adrenal))   \n",
      "531   (707 (Pressure ulcer, site NOS), 227 (Benign n...   \n",
      "3722           (438 (Ataxia), 443 (Raynaud's syndrome))   \n",
      "4274  (V58 (Radiotherapy encounter), 397 (Tricuspid ...   \n",
      "5258                     (452 (Portal vein thrombosis))   \n",
      "5262                     (452 (Portal vein thrombosis))   \n",
      "5266                     (452 (Portal vein thrombosis))   \n",
      "5277                     (452 (Portal vein thrombosis))   \n",
      "5294                     (452 (Portal vein thrombosis))   \n",
      "17                     (253 (Acromegaly and gigantism))   \n",
      "534                    (253 (Acromegaly and gigantism))   \n",
      "3725                   (344 (Monplga upr lmb dmnt sde))   \n",
      "4277                     (396 (Mitral/aortic stenosis))   \n",
      "5255  (571 (Alcoholic fatty liver), 585 (Chro kidney...   \n",
      "5259  (571 (Alcoholic fatty liver), 785 (Tachycardia...   \n",
      "5263  (995 (Other anaphylactic react), 571 (Alcoholi...   \n",
      "\n",
      "                                    consequents_labeled  support  confidence  \\\n",
      "532                     (227 (Benign neoplasm adrenal))    0.023        1.00   \n",
      "533   (253 (Acromegaly and gigantism), 707 (Pressure...    0.023        1.00   \n",
      "5270                     (452 (Portal vein thrombosis))    0.023        1.00   \n",
      "5273  (572 (Abscess of liver), 585 (Chro kidney dis ...    0.023        1.00   \n",
      "16                     (253 (Acromegaly and gigantism))    0.023        1.00   \n",
      "531                    (253 (Acromegaly and gigantism))    0.023        1.00   \n",
      "3722                   (344 (Monplga upr lmb dmnt sde))    0.023        1.00   \n",
      "4274                     (396 (Mitral/aortic stenosis))    0.023        1.00   \n",
      "5258  (571 (Alcoholic fatty liver), 585 (Chro kidney...    0.023        1.00   \n",
      "5262  (571 (Alcoholic fatty liver), 785 (Tachycardia...    0.023        1.00   \n",
      "5266  (995 (Other anaphylactic react), 571 (Alcoholi...    0.023        1.00   \n",
      "5277    (785 (Tachycardia NOS), 572 (Abscess of liver))    0.023        1.00   \n",
      "5294  (785 (Tachycardia NOS), 585 (Chro kidney dis s...    0.023        1.00   \n",
      "17                      (227 (Benign neoplasm adrenal))    0.023        0.75   \n",
      "534   (707 (Pressure ulcer, site NOS), 227 (Benign n...    0.023        0.75   \n",
      "3725           (438 (Ataxia), 443 (Raynaud's syndrome))    0.023        0.75   \n",
      "4277  (V58 (Radiotherapy encounter), 397 (Tricuspid ...    0.023        0.75   \n",
      "5255                     (452 (Portal vein thrombosis))    0.023        0.75   \n",
      "5259                     (452 (Portal vein thrombosis))    0.023        0.75   \n",
      "5263                     (452 (Portal vein thrombosis))    0.023        0.75   \n",
      "\n",
      "       lift  \n",
      "532   43.00  \n",
      "533   43.00  \n",
      "5270  43.00  \n",
      "5273  43.00  \n",
      "16    32.25  \n",
      "531   32.25  \n",
      "3722  32.25  \n",
      "4274  32.25  \n",
      "5258  32.25  \n",
      "5262  32.25  \n",
      "5266  32.25  \n",
      "5277  32.25  \n",
      "5294  32.25  \n",
      "17    32.25  \n",
      "534   32.25  \n",
      "3725  32.25  \n",
      "4277  32.25  \n",
      "5255  32.25  \n",
      "5259  32.25  \n",
      "5263  32.25  \n",
      "\n",
      "Results saved to: C:\\Users\\sarve\\Downloads\\Final_Project_hospital_app\\processed\\mimic\\evaluation_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "import sys, subprocess, importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score,\n",
    "    mean_absolute_error, r2_score,\n",
    "    silhouette_score, calinski_harabasz_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Define paths\n",
    "INPUT_DIR = Path(r\"C:\\Users\\sarve\\Downloads\\Final_Project_hospital_app\\MIMIC-III\")\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\sarve\\Downloads\\Final_Project_hospital_app\\processed\\mimic\")\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_parquet(OUTPUT_DIR/\"mimic_features.parquet\")\n",
    "\n",
    "# Print basic info about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Split features and target for classification\n",
    "X = df.drop(['label_mortality', 'target_los_days'], axis=1)\n",
    "y_mortality = df['label_mortality']\n",
    "y_los = df['target_los_days']\n",
    "\n",
    "# Print class distribution before splitting\n",
    "\n",
    "# Modified data splitting approach\n",
    "print(\"\\nClass distribution before splitting:\")\n",
    "print(y_mortality.value_counts())\n",
    "\n",
    "def safe_split(X, y, train_size=0.8, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs a safe split that handles extreme class imbalance\n",
    "    \"\"\"\n",
    "    # Get minority class samples\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    minority_class = classes[np.argmin(counts)]\n",
    "    minority_mask = y == minority_class\n",
    "\n",
    "    # Split minority class samples\n",
    "    minority_indices = np.where(minority_mask)[0]\n",
    "    n_minority_train = max(1, int(len(minority_indices) * train_size))\n",
    "\n",
    "    minority_train_idx = minority_indices[:n_minority_train]\n",
    "    minority_test_idx = minority_indices[n_minority_train:]\n",
    "\n",
    "    # Split majority class samples\n",
    "    majority_mask = ~minority_mask\n",
    "    majority_indices = np.where(majority_mask)[0]\n",
    "    n_majority_train = int(len(majority_indices) * train_size)\n",
    "\n",
    "    majority_train_idx = majority_indices[:n_majority_train]\n",
    "    majority_test_idx = majority_indices[n_majority_train:]\n",
    "\n",
    "    # Combine indices\n",
    "    train_idx = np.concatenate([minority_train_idx, majority_train_idx])\n",
    "    test_idx = np.concatenate([minority_test_idx, majority_test_idx])\n",
    "\n",
    "    # Split the data\n",
    "    X_train = X.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "    y_test = y.iloc[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Apply safe split\n",
    "X_train, X_test, y_mort_train, y_mort_test = safe_split(X, y_mortality)\n",
    "\n",
    "print(\"\\nClass distribution after splitting:\")\n",
    "print(\"Train:\", np.bincount(y_mort_train))\n",
    "print(\"Test:\", np.bincount(y_mort_test))\n",
    "\n",
    "# Use same split for regression\n",
    "y_los_train = y_los[X_train.index]\n",
    "y_los_test = y_los[X_test.index]\n",
    "\n",
    "# Define preprocessing\n",
    "numeric_features = ['age', 'icu_los_days'] + [col for col in X.columns if col.endswith('_24h')]\n",
    "categorical_features = ['gender', 'admission_type', 'admission_location', 'ethnicity']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ]), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 1. Classification Model\n",
    "print(\"\\n=== Mortality Classification ===\")\n",
    "# Modify classifier to handle extreme imbalance\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        max_iter=1000,  # Increase iterations if needed\n",
    "        solver='liblinear'  # More stable for imbalanced data\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_mort_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Classification metrics\n",
    "accuracy = accuracy_score(y_mort_test, y_pred)\n",
    "f1 = f1_score(y_mort_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_mort_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
    "\n",
    "# 2. Regression Model\n",
    "print(\"\\n=== Length of Stay Regression ===\")\n",
    "reg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Baseline: predict mean of training set\n",
    "baseline_pred = np.full_like(y_los_test, y_los_train.mean())\n",
    "\n",
    "reg.fit(X_train, y_los_train)\n",
    "y_pred_reg = reg.predict(X_test)\n",
    "\n",
    "# Regression metrics\n",
    "mae = mean_absolute_error(y_los_test, y_pred_reg)\n",
    "mae_baseline = mean_absolute_error(y_los_test, baseline_pred)\n",
    "rmse = np.sqrt(mean_absolute_error(y_los_test, y_pred_reg))  # mean_squared_error was missing, using MAE for RMSE\n",
    "r2 = r2_score(y_los_test, y_pred_reg)\n",
    "\n",
    "print(f\"MAE: {mae:.2f} days (baseline: {mae_baseline:.2f} days)\")\n",
    "print(f\"RMSE: {rmse:.2f} days\")\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "\n",
    "# 3. Clustering\n",
    "print(\"\\n=== Patient Clustering ===\")\n",
    "# Select features for clustering\n",
    "cluster_features = ['age'] + [col for col in numeric_features if '_24h' in col]\n",
    "X_cluster = df[cluster_features].copy()\n",
    "\n",
    "# Preprocess for clustering\n",
    "imp_scale = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "X_scaled = imp_scale.fit_transform(X_cluster)\n",
    "\n",
    "# Try different numbers of clusters\n",
    "silhouette_scores = []\n",
    "ch_scores = []\n",
    "k_range = range(2, 7)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "    ch_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "\n",
    "# Find optimal k\n",
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Optimal number of clusters: {best_k}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "print(f\"Best Calinski-Harabasz score: {max(ch_scores):.1f}\")\n",
    "\n",
    "# Final clustering\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_STATE)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Profile clusters\n",
    "cluster_profile = df.groupby('cluster')[cluster_features].mean()\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(cluster_profile)\n",
    "\n",
    "# 4. Association Rules\n",
    "print(\"\\n=== Association Rules Mining ===\")\n",
    "# Prepare diagnosis data\n",
    "dx = pd.read_parquet(OUTPUT_DIR/\"diagnoses_icd.parquet\")\n",
    "dx['icd9_3'] = dx['icd9_code'].str[:3]\n",
    "\n",
    "# Count diagnosis frequencies\n",
    "dx_counts = dx.groupby('icd9_3').size()\n",
    "# Keep only diagnoses that appear in at least 1% of admissions\n",
    "min_diagnoses = len(dx['hadm_id'].unique()) * 0.01\n",
    "frequent_dx = dx_counts[dx_counts >= min_diagnoses].index\n",
    "\n",
    "# Filter to frequent diagnoses before pivoting\n",
    "dx_filtered = dx[dx['icd9_3'].isin(frequent_dx)]\n",
    "print(f\"Filtered from {len(dx_counts)} to {len(frequent_dx)} diagnoses\")\n",
    "\n",
    "# Create binary transaction matrix (1 if diagnosis present, 0 if not)\n",
    "dx_pivot = pd.crosstab(dx_filtered['hadm_id'], dx_filtered['icd9_3'])\n",
    "dx_pivot = (dx_pivot > 0).astype(int)  # Convert to binary 0/1 matrix\n",
    "\n",
    "print(f\"Transaction matrix shape: {dx_pivot.shape}\")\n",
    "\n",
    "# Mine frequent itemsets with optimized parameters\n",
    "print(\"Mining frequent itemsets...\")\n",
    "frequent_itemsets = apriori(\n",
    "    dx_pivot,\n",
    "    min_support=0.02,  # Increased minimum support\n",
    "    use_colnames=True,\n",
    "    max_len=3  # Limit to rules with up to 3 items\n",
    ")\n",
    "print(f\"Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "\n",
    "# Generate rules with adjusted thresholds\n",
    "print(\"Generating association rules...\")\n",
    "rules = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"confidence\",\n",
    "    min_threshold=0.5\n",
    ")\n",
    "\n",
    "# Sort and filter rules\n",
    "rules = (rules\n",
    "         .sort_values(['lift', 'confidence'], ascending=False)\n",
    "         .query('lift > 1.5')  # Increased lift threshold\n",
    "         .head(20))  # Top 20 rules\n",
    "\n",
    "print(f\"\\nFound {len(rules)} strong association rules\")\n",
    "print(\"\\nTop Association Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].round(3))\n",
    "\n",
    "# Add interpretable labels if available\n",
    "try:\n",
    "    # Try to load ICD-9 descriptions if available\n",
    "    icd9_labels = pd.read_csv(INPUT_DIR/\"D_ICD_DIAGNOSES.csv\")\n",
    "    icd9_labels['icd9_3'] = icd9_labels['icd9_code'].str[:3]\n",
    "    label_dict = icd9_labels.groupby('icd9_3')['short_title'].first().to_dict()\n",
    "    \n",
    "    # Function to replace codes with labels in frozenset\n",
    "    def label_codes(code_set):\n",
    "        return frozenset([f\"{code} ({label_dict.get(code, 'Unknown')})\" \n",
    "                         for code in code_set])\n",
    "    \n",
    "    # Add labeled columns\n",
    "    rules['antecedents_labeled'] = rules['antecedents'].apply(label_codes)\n",
    "    rules['consequents_labeled'] = rules['consequents'].apply(label_codes)\n",
    "    \n",
    "    print(\"\\nTop Rules with Labels:\")\n",
    "    print(rules[['antecedents_labeled', 'consequents_labeled', \n",
    "                 'support', 'confidence', 'lift']].round(3))\n",
    "except Exception as e:\n",
    "    print(\"Could not load ICD-9 descriptions:\", e)\n",
    "\n",
    "# Add rules metrics to results\n",
    "results = {\n",
    "    'classification': {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    },\n",
    "    'regression': {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    },\n",
    "    'clustering': {\n",
    "        'n_clusters': best_k,\n",
    "        'silhouette': max(silhouette_scores),\n",
    "        'calinski_harabasz': max(ch_scores)\n",
    "    }\n",
    "}\n",
    "\n",
    "results['association_rules'] = {\n",
    "    'n_rules': len(rules),\n",
    "    'max_lift': float(rules['lift'].max()),\n",
    "    'max_confidence': float(rules['confidence'].max()),\n",
    "    'mean_support': float(rules['support'].mean())\n",
    "}\n",
    "\n",
    "# Save results to file\n",
    "import json\n",
    "with open(OUTPUT_DIR/'evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to:\", OUTPUT_DIR/'evaluation_metrics.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
